import json, os, torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split, KFold
from transformers import (
    AutoTokenizer, AutoModelForTokenClassification,
    TrainingArguments, Trainer, DataCollatorForTokenClassification,
    EarlyStoppingCallback
)
from datasets import Dataset
from collections import defaultdict
from transformers import Trainer
from seqeval.metrics import classification_report as seq_classification_report
from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score
import random



# ğŸ”§ ê²½ë¡œ ì„¤ì •
doccano_file = "/final_biobert/ner_training_data_biobert.jsonl"
base_dir = "C:/Users/KDT_35/PycharmProjects/Oracle_Final_Project/DB-BioBERT pipeline/ner_biobert_model_epoch_2_kfold_3_sampling"
ENTITY_TYPES = ["ê· ëª…", "ê· ìˆ˜ì¹˜", "í•­ìƒì œëª…", "MICê²°ê³¼", "ê°ìˆ˜ì„±ê²°ê³¼"]

# âœ… 1. Doccano ë°ì´í„° ë¡œë“œ
def get_bio_labels(text, labels):
    tags = ["O"] * len(text)
    for start, end, label in labels:
        tags[start] = f"B-{label}"
        for i in range(start + 1, end):
            tags[i] = f"I-{label}"
    return tags

examples = []
with open(doccano_file, "r", encoding="utf-8-sig") as f:
    for line in f:
        item = json.loads(line)
        text = item["text"]
        bio_labels = get_bio_labels(text, item["labels"])
        examples.append({"tokens": list(text), "ner_tags": bio_labels})

# ì „ì²´ ë°ì´í„°ì—ì„œ 20ë§Œê°œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§
SAMPLE_SIZE = 200_000  # ì›í•˜ëŠ” ìƒ˜í”Œ ìˆ˜
random.seed(42)  # ì¬í˜„ì„±

if len(examples) > SAMPLE_SIZE:
    examples = random.sample(examples, SAMPLE_SIZE)
else:
    print(f"ë°ì´í„°ê°€ {SAMPLE_SIZE}ë³´ë‹¤ ì ìœ¼ë¯€ë¡œ ì „ì²´ {len(examples)}ê°œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# âœ… 2. train/val/test ë¶„ë¦¬ (90:10)
trainval_examples, test_examples = train_test_split(examples, test_size=0.1, random_state=42)

# âœ… 3. tokenizer ë° ë¼ë²¨ ì„¤ì •
model_name = "dmis-lab/biobert-base-cased-v1.1"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
label_list = ['O'] + [f"{p}-{l}" for l in ENTITY_TYPES for p in ['B', 'I']]
label_to_id = {label: i for i, label in enumerate(label_list)}
id_to_label = {i: label for label, i in label_to_id.items()}

def tokenize_and_align_labels(example):
    tokenized_input = tokenizer(example["tokens"], is_split_into_words=True, truncation=True, max_length=512, padding='max_length')
    word_ids = tokenized_input.word_ids()
    aligned_labels = []
    prev_word_idx = None
    for word_idx in word_ids:
        if word_idx is None:
            aligned_labels.append(-100)
        elif word_idx != prev_word_idx:
            aligned_labels.append(label_to_id.get(example["ner_tags"][word_idx], 0))
        else:
            aligned_labels.append(-100)
        prev_word_idx = word_idx
    tokenized_input["labels"] = aligned_labels
    return tokenized_input

# âœ… 4. Dataset ìƒì„± ë° í† í°í™”
dataset_trainval = Dataset.from_list(trainval_examples).map(tokenize_and_align_labels)
dataset_test = Dataset.from_list(test_examples).map(tokenize_and_align_labels)

# âœ… 5. 3-Fold êµì°¨ê²€ì¦ í•™ìŠµ
kf = KFold(n_splits=3, shuffle=True, random_state=42)
for fold, (train_idx, val_idx) in enumerate(kf.split(dataset_trainval)):
    print(f"\n===== ğŸŒ€ Fold {fold+1} / 3 ì‹œì‘ =====")
    train_dataset = dataset_trainval.select(train_idx)
    eval_dataset = dataset_trainval.select(val_idx)

    model = AutoModelForTokenClassification.from_pretrained(
        model_name,
        num_labels=len(label_list),
        id2label=id_to_label,
        label2id=label_to_id
    )

    fold_dir = os.path.join(base_dir, f"fold_{fold+1}")
    os.makedirs(fold_dir, exist_ok=True)

    # âœ… fold_dir ì •ì˜ ì´í›„ ì¶”ê°€
    final_model_dir = os.path.join(fold_dir, "final_model")  # âœ… ìˆ˜ì •ë¨: í•˜ìœ„ì— final_model í´ë” ìƒì„±
    os.makedirs(final_model_dir, exist_ok=True)

    def compute_metrics(p):
        predictions, labels = p
        preds = np.argmax(predictions, axis=2)

        true_labels = []
        true_preds = []

        for pred, label in zip(preds, labels):
            temp_pred = []
            temp_label = []
            for p_, l_ in zip(pred, label):
                if l_ != -100:
                    temp_pred.append(id_to_label[p_])
                    temp_label.append(id_to_label[l_])
            true_preds.append(temp_pred)
            true_labels.append(temp_label)

        return {
            "f1": f1_score(true_labels, true_preds),
            "accuracy": accuracy_score(true_labels, true_preds),
            "precision": precision_score(true_labels, true_preds),
            "recall": recall_score(true_labels, true_preds)
        }

    training_args = TrainingArguments(
        output_dir=fold_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        logging_dir=os.path.join(fold_dir, "logs"),
        logging_steps=10,
        logging_strategy="steps",  # âœ… ê¼­ í•„ìš”
        overwrite_output_dir=True,
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForTokenClassification(tokenizer),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
        compute_metrics=compute_metrics
    )

    trainer.train()
    # âœ… ìˆ˜ì •ë¨: ì‹¤ì œë¡œ ì‚¬ìš©í•  ëª¨ë¸ì€ final_model ë””ë ‰í† ë¦¬ì— ì €ì¥
    trainer.model.save_pretrained(final_model_dir)  # âœ… best model ì €ì¥
    trainer.tokenizer.save_pretrained(final_model_dir)  # âœ… tokenizer ì €ì¥

    # fold í‰ê°€ (eval_dataset ê¸°ì¤€)
    predictions, labels, _ = trainer.predict(eval_dataset)
    preds = np.argmax(predictions, axis=2)

    true_labels, true_preds = [], []
    for pred, label in zip(preds, labels):
        sent_true, sent_pred = [], []
        for p_, l_ in zip(pred, label):
            if l_ != -100:
                sent_true.append(id_to_label[l_])
                sent_pred.append(id_to_label[p_])
        true_labels.append(sent_true)
        true_preds.append(sent_pred)

    # ğŸ“Š ì—”í‹°í‹°ë³„ ì„±ëŠ¥ ë¶„ì„
    report = seq_classification_report(true_labels, true_preds, output_dict=True)
    entity_scores = defaultdict(lambda: {"precision": [], "recall": [], "f1": []})

    def extract_entity(bio):
        return bio.split("-")[1] if "-" in bio else "O"

    for label, metrics in seq_classification_report(true_labels, true_preds, output_dict=True).items():
      if label in ["accuracy", "macro avg", "weighted avg"]:
        continue
      entity = label.split("-")[1] if "-" in label else "O"
      entity_scores[entity]["precision"].append(metrics["precision"])
      entity_scores[entity]["recall"].append(metrics["recall"])
      entity_scores[entity]["f1"].append(metrics["f1-score"])

    entity_avg_report = {
        ent: {
            "precision": np.mean(scores["precision"]),
            "recall": np.mean(scores["recall"]),
            "f1": np.mean(scores["f1"])
        }
        for ent, scores in entity_scores.items()
    }

    # ì €ì¥
    with open(os.path.join(fold_dir, "entity_wise_report.json"), "w", encoding="utf-8") as f:
        json.dump(entity_avg_report, f, ensure_ascii=False, indent=2)


# âœ… 6. Fold 3ì˜ Best Model ë¶ˆëŸ¬ì™€ì„œ test set í‰ê°€
print("\nğŸ§ª Fold 3ì˜ Best Modelë¡œ test set í‰ê°€ ì¤‘...")

# fold_3 ë””ë ‰í† ë¦¬ì—ì„œ best model ë¶ˆëŸ¬ì˜¤ê¸°
best_model_dir = os.path.join(base_dir, "fold_3", "final_model")
model = AutoModelForTokenClassification.from_pretrained(best_model_dir)
tokenizer = AutoTokenizer.from_pretrained(best_model_dir)

# ìƒˆ Trainer ìƒì„± (í‰ê°€ë§Œ ìˆ˜í–‰)
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    data_collator=DataCollatorForTokenClassification(tokenizer)
)

# ì˜ˆì¸¡
predictions, labels, _ = trainer.predict(dataset_test)
preds = np.argmax(predictions, axis=2)

# ë¼ë²¨ ë³µì›
# ğŸ¤– id_to_labelì€ ê¸°ì¡´ì— í•™ìŠµí•œ tokenizer.config ë˜ëŠ” label_list ê¸°ë°˜ìœ¼ë¡œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨
true_labels, true_preds = [], []

# labelsì™€ predsëŠ” model.predict() ê²°ê³¼ì—ì„œ ì–»ì€ logits ë˜ëŠ” prediction label list
for pred, label in zip(preds, labels):  # preds: List[List[int]], labels: List[List[int]]
    sent_true, sent_pred = [], []
    for p_, l_ in zip(pred, label):
        if l_ != -100:  # ignore special tokens
            sent_true.append(id_to_label[l_])  # ì˜ˆ: 'B-Bacteria', 'O', ...
            sent_pred.append(id_to_label[p_])
    true_labels.append(sent_true)
    true_preds.append(sent_pred)

# ì—”í‹°í‹°ë³„ ì„±ëŠ¥ ë¶„ì„
report = seq_classification_report(true_labels, true_preds, output_dict=True)

# âœ… 7. ì„±ëŠ¥ ì €ì¥
final_report = seq_classification_report(true_labels, true_preds, digits=4)
report_path = os.path.join(base_dir, "final_test_classification_report.txt")
with open(report_path, "w", encoding="utf-8") as f:
    f.write(final_report)

print("âœ… ìµœì¢… test ì„±ëŠ¥ ì €ì¥ ì™„ë£Œ!\n")
print(final_report)

# âœ… 8. test ì—”í‹°í‹°ë³„ ì„±ëŠ¥ ì €ì¥ (JSON)
test_entity_scores = defaultdict(lambda: {"precision": [], "recall": [], "f1": []})

for label, metrics in report.items():
    if label in ["accuracy", "macro avg", "weighted avg"]:
        continue
    entity = label.split("-")[1] if "-" in label else "O"
    test_entity_scores[entity]["precision"].append(metrics["precision"])
    test_entity_scores[entity]["recall"].append(metrics["recall"])
    test_entity_scores[entity]["f1"].append(metrics["f1-score"])

test_entity_avg = {
    entity: {
        "precision": np.mean(score["precision"]),
        "recall": np.mean(score["recall"]),
        "f1": np.mean(score["f1"])
    }
    for entity, score in test_entity_scores.items()
}

entity_json_path = os.path.join(base_dir, "final_test_entity_report.json")
with open(entity_json_path, "w", encoding="utf-8") as f:
    json.dump(test_entity_avg, f, ensure_ascii=False, indent=2)

print("\u2705 test ì—”zí‹°í‹°ë³„ ì„±ëŠ¥ JSON ì €ì¥ ì™„ë£Œ")

# âœ… 9. test ì˜ˆì¸¡ ê²°ê³¼ csv ì €ì¥
flat_tokens, flat_true, flat_pred = [], [], []
for i, ex in enumerate(dataset_test):
    tokens = test_examples[i]["tokens"]
    token_idx = 0
    for j, word_id in enumerate(ex['labels']):
        if word_id != -100:  # âœ… ìˆ˜ì •ë¨: ì‹¤ì œ í† í°ê³¼ ë¼ë²¨ ë§¤í•‘
            if token_idx < len(tokens):
                flat_tokens.append(tokens[token_idx])
            else:
                flat_tokens.append("[UNK]")  # âœ… IndexError ë°©ì§€

            flat_true.append(id_to_label[ex['labels'][j]])
            flat_pred.append(id_to_label[preds[i][j]])
            token_idx += 1
        # if word_id != -100:
        #     flat_tokens.append(tokens[token_idx])
        #     flat_true.append(id_to_label[ex['labels'][j]])
        #     flat_pred.append(id_to_label[preds[i][j]])
        #     token_idx += 1

df_test_results = pd.DataFrame({
    "token": flat_tokens,
    "true_label": flat_true,
    "pred_label": flat_pred
})
csv_path = os.path.join(base_dir, "final_test_predictions.csv")
df_test_results.to_csv(csv_path, index=False, encoding="utf-8-sig")

print("âœ… test ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ:", csv_path)